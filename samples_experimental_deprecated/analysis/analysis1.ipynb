{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production-Grade Data Analysis Notebook\n",
    "\n",
    "**Table:** `dm.sales_order_item_flat`  \n",
    "**Database URL:** `postgresql+psycopg2://postgres:pg123@db-postgres:5432/data_warehouse`\n",
    "\n",
    **Description:**\n",
    "This notebook performs a full, production-grade data analysis pipeline on the `dm.sales_order_item_flat` table. Since this table has already been cleaned, deduplicated, and validated by your upstream ETL, we only focus on:\n",
    "1. Connecting to Postgres via SQLAlchemy\n",
    "2. Loading data into Pandas efficiently\n",
    "3. Exploratory Data Analysis (EDA)\n",
    "4. Feature Engineering (date-based features, RFM segmentation)\n",
    "5. Advanced analyses (sales trends, top customers, top products, customer lifetime value)\n",
    "6. Visualizations (static charts via Matplotlib/Seaborn)\n",
    "7. Saving outputs (CSV/Parquet and writing summary tables back to Postgres)\n",
    "\n",
    "**How to run:**\n",
    "- Ensure you are in a Python 3.7+ virtual environment and have installed:\n",
    "  ```bash\n",
    "  pip install pandas sqlalchemy psycopg2-binary matplotlib seaborn scikit-learn\n",
    "  ```\n",
    "- Open this notebook in JupyterLab or VS Code (with Jupyter extension) and execute cells sequentially.\n",
    "- Adjust any parameters in the “Parameters” cell as needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parameters & Configuration\n",
    "\n",
    "Define global parameters that can be overridden if you later parameterize this notebook with Papermill or similar.  \n",
    "- `DB_URL`: SQLAlchemy connection string  \n",
    "- `OUTPUT_DIR`: directory to save any CSV/Parquet outputs  \n",
    "- `SUMMARY_TABLE`: name of the summary table to write back into Postgres  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (override via Papermill if desired)\n",
    "DB_URL        = \"postgresql+psycopg2://postgres:pg123@db-postgres:5432/data_warehouse\"\n",
    "OUTPUT_DIR    = \"./outputs\"\n",
    "SUMMARY_TABLE = \"monthly_sales_summary\"\n",
    "SCHEMA        = \"dm\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "import os\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports & Environment Setup\n",
    "\n",
    "Import required libraries, configure plotting styles, and set up logging for a production environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import sys\n",
    "from sqlalchemy import create_engine, text\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Configure Seaborn/Matplotlib\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "# Suppress non-critical warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Logging Configuration\n",
    "\n",
    "Initialize a logger to capture INFO and ERROR messages.  \n",
    "Logs will be written both to console and to a file under `OUTPUT_DIR`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logger\n",
    "logger = logging.getLogger(\"sales_analysis\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\n",
    "    fmt=\"%(asctime)s — %(name)s — %(levelname)s — %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# Console handler\n",
    "console_handler = logging.StreamHandler(sys.stdout)\n",
    "console_handler.setLevel(logging.INFO)\n",
    "console_handler.setFormatter(formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# File handler\n",
    "log_file = os.path.join(OUTPUT_DIR, \"sales_analysis.log\")\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "logger.info(\"Logger initialized.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Database Connection\n",
    "\n",
    "Create a SQLAlchemy engine with `pool_pre_ping=True` to ensure connections remain valid.  \n",
    "Wrap in a function to handle exceptions gracefully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_engine(db_url: str):\n",
    "    \"\"\"\n",
    "    Create and return a SQLAlchemy engine with minimal retry logic.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        engine = create_engine(db_url, pool_pre_ping=True)\n",
    "        # Test connection\n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(text(\"SELECT 1\"))\n",
    "        logger.info(\"Successfully connected to Postgres.\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Failed to connect to Postgres.\")\n",
    "        raise\n",
    "\n",
    "# Instantiate engine\n",
    "engine = get_engine(DB_URL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Data into Pandas\n",
    "\n",
    "Since the ETL pipeline already loaded a day's worth of cleaned data into `dm.sales_order_item_flat`, we can load the entire table into memory.  \n",
    "If this table grows large, consider adding `chunksize=` and reading in increments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full table into a DataFrame\n",
    "try:\n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "        item_id,\n",
    "        order_id,\n",
    "        order_number,\n",
    "        order_created_at,\n",
    "        order_total,\n",
    "        total_qty_ordered,\n",
    "        customer_id,\n",
    "        customer_name,\n",
    "        customer_gender,\n",
    "        customer_email,\n",
    "        product_id,\n",
    "        product_sku,\n",
    "        product_name,\n",
    "        item_price,\n",
    "        item_qty_order,\n",
    "        item_unit_total\n",
    "    FROM dm.sales_order_item_flat;\n",
    "    \"\"\"\n",
    "    df = pd.read_sql_query(text(query), engine, parse_dates=[\"order_created_at\"])\n",
    "    logger.info(f\"Loaded {len(df)} rows from dm.sales_order_item_flat.\")\n",
    "except Exception as e:\n",
    "    logger.exception(\"Error loading data from Postgres.\")\n",
    "    raise\n",
    "\n",
    "# Display DataFrame info and first few rows\n",
    "logger.info(f\"DataFrame shape: {df.shape}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Perform high-level EDA to understand distributions, unique counts, and basic statistics.  \n",
    "Since data is already validated, we skip null checks and business-rule checks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Basic Summary Statistics\n",
    "numeric_cols = [\n",
    "    \"order_total\", \"total_qty_ordered\", \"item_price\", \"item_qty_order\", \"item_unit_total\"\n",
    "]\n",
    "numeric_summary = df[numeric_cols].describe().T\n",
    "logger.info(\"Numeric summary statistics:\")\n",
    "logger.info(f\"\\n{numeric_summary}\")\n",
    "\n",
    "# 6.2 Unique Counts\n",
    "n_orders = df[\"order_id\"].nunique()\n",
    "n_customers = df[\"customer_id\"].nunique()\n",
    "n_products = df[\"product_id\"].nunique()\n",
    "logger.info(f\"Unique orders: {n_orders}\")\n",
    "logger.info(f\"Unique customers: {n_customers}\")\n",
    "logger.info(f\"Unique products: {n_products}\")\n",
    "\n",
    "# 6.3 Distribution of customer_gender\n",
    "gender_counts = df[\"customer_gender\"].value_counts()\n",
    "logger.info(f\"Customer gender distribution:\\n{gender_counts}\")\n",
    "\n",
    "# 6.4 Top 5 Customers by Number of Orders\n",
    "top5_customers = (\n",
    "    df.groupby([\"customer_id\", \"customer_name\"])\n",
    "      .agg(num_orders=(\"order_id\", \"nunique\"))\n",
    "      .reset_index()\n",
    "      .sort_values(\"num_orders\", ascending=False)\n",
    "      .head(5)\n",
    ")\n",
    "logger.info(\"Top 5 customers by number of orders:\")\n",
    "logger.info(f\"\\n{top5_customers}\")\n",
    "\n",
    "# 6.5 Histograms of Key Metrics\n",
    "plt.figure()\n",
    "sns.histplot(df[\"order_total\"], bins=50, kde=True)\n",
    "plt.title(\"Distribution of Order Total\")\n",
    "plt.xlabel(\"Order Total ($)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "sns.histplot(df[\"item_unit_total\"], bins=50, kde=True)\n",
    "plt.title(\"Distribution of Item Unit Total\")\n",
    "plt.xlabel(\"Item Unit Total ($)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering\n",
    "\n",
    "Create derived features:\n",
    "1. Date-based: `order_date`, `order_month`, `order_day_of_week`, `order_hour`  \n",
    "2. RFM (Recency, Frequency, Monetary) metrics at the customer level  \n",
    "3. Label encoding for gender (for potential modeling)  \n",
    "4. Email domain extraction  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Date-Based Features\n",
    "df[\"order_date\"] = df[\"order_created_at\"].dt.date\n",
    "df[\"order_month\"] = df[\"order_created_at\"].dt.to_period(\"M\").astype(str)\n",
    "df[\"order_day_of_week\"] = df[\"order_created_at\"].dt.day_name()\n",
    "df[\"order_hour\"] = df[\"order_created_at\"].dt.hour\n",
    "\n",
    "# 7.2 RFM Metrics Calculation\n",
    "max_date = df[\"order_created_at\"].max().normalize()\n",
    "logger.info(f\"Max order timestamp: {max_date}\")\n",
    "\n",
    "rfm = (\n",
    "    df.groupby([\"customer_id\", \"customer_name\"]).agg(\n",
    "        last_order_date=(\"order_created_at\", \"max\"),\n",
    "        frequency=(\"order_id\", \"nunique\"),\n",
    "        monetary=(\"item_unit_total\", \"sum\")\n",
    "    ).reset_index()\n",
    ")\n",
    "rfm[\"recency_days\"] = (max_date - rfm[\"last_order_date\"].dt.normalize()).dt.days\n",
    "\n",
    "# 7.3 RFM Scoring (1–5 Quintiles)\n",
    "def assign_r_score(x, quantiles):\n",
    "    # Lower recency_days is better, so invert\n",
    "    return 5 - int(pd.qcut([x], q=quantiles, labels=False, duplicates=\"drop\")[0])\n",
    "\n",
    "def assign_fm_score(x, quantiles):\n",
    "    # Higher is better\n",
    "    return int(pd.qcut([x], q=quantiles, labels=False, duplicates=\"drop\")[0]) + 1\n",
    "\n",
    "quantiles = 5\n",
    "rfm[\"R_score\"] = rfm[\"recency_days\"].apply(lambda x: assign_r_score(x, quantiles))\n",
    "rfm[\"F_score\"] = rfm[\"frequency\"].apply(lambda x: assign_fm_score(x, quantiles))\n",
    "rfm[\"M_score\"] = rfm[\"monetary\"].apply(lambda x: assign_fm_score(x, quantiles))\n",
    "\n",
    "rfm[\"RFM_score\"] = rfm[\"R_score\"] * 100 + rfm[\"F_score\"] * 10 + rfm[\"M_score\"]\n",
    "\n",
    "rfm.head()\n",
    "\n",
    "# 7.4 Email Domain Extraction\n",
    "df[\"email_domain\"] = df[\"customer_email\"].str.split(\"@\").str[-1].str.lower()\n",
    "\n",
    "# 7.5 Encode Gender for Modeling\n",
    "le_gender = LabelEncoder()\n",
    "df[\"gender_encoded\"] = le_gender.fit_transform(df[\"customer_gender\"].fillna(\"Unknown\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. RFM Segmentation\n",
    "\n",
    "Bucket customers into high-level segments (e.g., “Champions”, “Loyal Customers”, etc.) based on RFM scores.  \n",
    "Adjust the threshold logic to match your business rules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfm_segment(rfm_score):\n",
    "    \"\"\"\n",
    "    Map RFM_score to descriptive segments. Adjust thresholds as needed.\n",
    "    \"\"\"\n",
    "    if rfm_score >= 555:\n",
    "        return \"Champions\"\n",
    "    elif rfm_score >= 400:\n",
    "        return \"Loyal Customers\"\n",
    "    elif rfm_score >= 250:\n",
    "        return \"Potential Loyalist\"\n",
    "    elif rfm_score >= 100:\n",
    "        return \"Need Attention\"\n",
    "    else:\n",
    "        return \"At Risk\"\n",
    "\n",
    "rfm[\"Segment\"] = rfm[\"RFM_score\"].apply(rfm_segment)\n",
    "rfm[\"Segment\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Sales Trend Analysis\n",
    "\n",
    "Aggregate sales metrics over monthly and weekly periods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1 Monthly Sales\n",
    "sales_monthly = (\n",
    "    df.groupby(\"order_month\").agg(\n",
    "        total_sales=(\"item_unit_total\", \"sum\"),\n",
    "        total_orders=(\"order_id\", \"nunique\"),\n",
    "        total_qty=(\"item_qty_order\", \"sum\")\n",
    "    ).reset_index()\n",
    ")\n",
    "# Convert 'order_month' string to Timestamp for plotting\n",
    "sales_monthly[\"order_month_ts\"] = pd.to_datetime(sales_monthly[\"order_month\"] + \"-01\")\n",
    "\n",
    "sales_monthly.head()\n",
    "\n",
    "# 9.2 Weekly Sales\n",
    "df[\"order_week\"] = df[\"order_created_at\"].dt.to_period(\"W\").astype(str)\n",
    "sales_weekly = (\n",
    "    df.groupby(\"order_week\").agg(\n",
    "        total_sales=(\"item_unit_total\", \"sum\"),\n",
    "        total_orders=(\"order_id\", \"nunique\")\n",
    "    ).reset_index()\n",
    ")\n",
    "# Extract week start date for plotting\n",
    "sales_weekly[\"order_week_start\"] = pd.to_datetime(sales_weekly[\"order_week\"].str.split(\"/\").str[0])\n",
    "\n",
    "sales_weekly.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Product-Level Analysis\n",
    "\n",
    "Identify top products by quantity sold and by revenue, and examine price distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 Top 10 Products by Quantity Sold\n",
    "top_products_qty = (\n",
    "    df.groupby([\"product_id\", \"product_name\"]).agg(\n",
    "        total_qty_sold=(\"item_qty_order\", \"sum\")\n",
    "    ).reset_index()\n",
    "    .sort_values(\"total_qty_sold\", ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "top_products_qty\n",
    "\n",
    "# 10.2 Top 10 Products by Revenue\n",
    "top_products_rev = (\n",
    "    df.groupby([\"product_id\", \"product_name\"]).agg(\n",
    "        total_revenue=(\"item_unit_total\", \"sum\")\n",
    "    ).reset_index()\n",
    "    .sort_values(\"total_revenue\", ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "top_products_rev\n",
    "\n",
    "# 10.3 Price Distribution Boxplot\n",
    "plt.figure()\n",
    "sns.boxplot(x=df[\"item_price\"])\n",
    "plt.title(\"Boxplot of Item Price\")\n",
    "plt.xlabel(\"Item Price ($)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Customer Demographics & Behavior\n",
    "\n",
    "Analyze gender distribution, email domain popularity, and average order metrics by gender.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.1 Gender Distribution Pie Chart\n",
    "gender_counts = df[\"customer_gender\"].value_counts().reset_index()\n",
    "gender_counts.columns = [\"Gender\", \"Count\"]\n",
    "\n",
    "plt.figure()\n",
    "plt.pie(\n",
    "    gender_counts[\"Count\"],\n",
    "    labels=gender_counts[\"Gender\"],\n",
    "    autopct=\"%1.1f%%\",\n",
    "    startangle=140\n",
    ")\n",
    "plt.title(\"Customer Gender Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 11.2 Top 10 Email Domains\n",
    "top_domains = (\n",
    "    df[\"email_domain\"].value_counts()\n",
    "      .head(10)\n",
    "      .reset_index()\n",
    "      .rename(columns={\"index\": \"Domain\", \"email_domain\": \"Count\"})\n",
    ")\n",
    "top_domains\n",
    "\n",
    "plt.figure()\n",
    "sns.barplot(x=\"Count\", y=\"Domain\", data=top_domains, palette=\"crest\")\n",
    "plt.title(\"Top 10 Customer Email Domains\")\n",
    "plt.xlabel(\"Number of Customers\")\n",
    "plt.ylabel(\"Email Domain\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 11.3 Average Order Total by Gender\n",
    "avg_order_by_gender = (\n",
    "    df.groupby(\"customer_gender\").agg(\n",
    "        avg_order_total=(\"order_total\", \"mean\"),\n",
    "        num_orders=(\"order_id\", \"nunique\")\n",
    "    ).reset_index()\n",
    ")\n",
    "avg_order_by_gender\n",
    "\n",
    "plt.figure()\n",
    "sns.barplot(x=\"customer_gender\", y=\"avg_order_total\", data=avg_order_by_gender, palette=\"pastel\")\n",
    "plt.title(\"Average Order Total by Gender\")\n",
    "plt.ylabel(\"Average Order Total ($)\")\n",
    "plt.xlabel(\"Gender\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Customer Lifetime Value (CLTV) Approximation\n",
    "\n",
    "Compute a simple CLTV proxy: average order value × frequency.  \n",
    "List top customers by CLTV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12.1 Calculate CLTV Proxy\n",
    "rfm[\"avg_order_value\"] = rfm[\"monetary\"] / rfm[\"frequency\"]\n",
    "rfm[\"cltv_score\"] = rfm[\"avg_order_value\"] * rfm[\"frequency\"]\n",
    "\n",
    "top_cltv = rfm.sort_values(\"cltv_score\", ascending=False).head(10)\n",
    "top_cltv[[\"customer_id\", \"customer_name\", \"cltv_score\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Combined Visual Dashboard (Optional)\n",
    "\n",
    "Create a 2×2 grid of key charts:\n",
    "1. Monthly Sales Trend  \n",
    "2. Top 5 Products by Revenue  \n",
    "3. RFM Segment Distribution  \n",
    "4. Top 5 Customers by CLTV  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13.1 Prepare Data for Plots\n",
    "top5_prod_rev = top_products_rev.head(5)\n",
    "segment_counts = rfm[\"Segment\"].value_counts().reset_index()\n",
    "segment_counts.columns = [\"Segment\", \"Count\"]\n",
    "top5_cltv = top_cltv[[\"customer_name\", \"cltv_score\"]].head(5)\n",
    "\n",
    "# Convert order_month for plotting\n",
    "sales_monthly_plot = sales_monthly.copy()\n",
    "sales_monthly_plot[\"order_month_ts\"] = pd.to_datetime(sales_monthly_plot[\"order_month_ts\"])\n",
    "\n",
    "# 13.2 Plot Grid\n",
    "fig, axes = plt.subplots(2, 2, figsize=(24, 16))\n",
    "\n",
    "# A: Monthly Sales Trend\n",
    "sns.lineplot(\n",
    "    x=\"order_month_ts\", y=\"total_sales\", data=sales_monthly_plot,\n",
    "    marker=\"o\", ax=axes[0, 0], color=\"steelblue\"\n",
    ")\n",
    "axes[0, 0].set_title(\"Monthly Sales Trend\")\n",
    "axes[0, 0].set_xlabel(\"Month\")\n",
    "axes[0, 0].set_ylabel(\"Total Sales ($)\")\n",
    "axes[0, 0].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# B: Top 5 Products by Revenue\n",
    "sns.barplot(\n",
    "    x=\"total_revenue\", y=\"product_name\", data=top5_prod_rev.sort_values(\"total_revenue\", ascending=True),\n",
    "    ax=axes[0, 1], palette=\"crest\"\n",
    ")\n",
    "axes[0, 1].set_title(\"Top 5 Products by Revenue\")\n",
    "axes[0, 1].set_xlabel(\"Revenue ($)\")\n",
    "axes[0, 1].set_ylabel(\"Product Name\")\n",
    "\n",
    "# C: RFM Segment Distribution\n",
    "sns.barplot(\n",
    "    x=\"Count\", y=\"Segment\", data=segment_counts.sort_values(\"Count\", ascending=True),\n",
    "    ax=axes[1, 0], palette=\"Spectral\"\n",
    ")\n",
    "axes[1, 0].set_title(\"Customer RFM Segment Distribution\")\n",
    "axes[1, 0].set_xlabel(\"Number of Customers\")\n",
    "axes[1, 0].set_ylabel(\"Segment\")\n",
    "\n",
    "# D: Top 5 Customers by CLTV\n",
    "sns.barplot(\n",
    "    x=\"cltv_score\", y=\"customer_name\", data=top5_cltv.sort_values(\"cltv_score\", ascending=True),\n",
    "    ax=axes[1, 1], palette=\"magma\"\n",
    ")\n",
    "axes[1, 1].set_title(\"Top 5 Customers by CLTV\")\n",
    "axes[1, 1].set_xlabel(\"CLTV Score\")\n",
    "axes[1, 1].set_ylabel(\"Customer Name\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save Outputs\n",
    "\n",
    "1. Save cleaned DataFrame (if needed) as Parquet for archiving.  \n",
    "2. Save RFM summary to CSV.  \n",
    "3. Save time-series summaries (monthly & weekly) to CSV.  \n",
    "4. Write monthly sales summary back to Postgres for dashboard consumption.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14.1 Save cleaned full dataset as Parquet\n",
    "cleaned_path = os.path.join(OUTPUT_DIR, \"sales_order_item_flat_full.parquet\")\n",
    "df.to_parquet(cleaned_path, index=False)\n",
    "logger.info(f\"Saved full dataset to Parquet: {cleaned_path}\")\n",
    "\n",
    "# 14.2 Save RFM summary to CSV\n",
    "rfm_path = os.path.join(OUTPUT_DIR, \"customer_rfm.csv\")\n",
    "rfm.to_csv(rfm_path, index=False)\n",
    "logger.info(f\"Saved RFM summary to CSV: {rfm_path}\")\n",
    "\n",
    "# 14.3 Save sales_monthly and sales_weekly to CSV\n",
    "sales_monthly_path = os.path.join(OUTPUT_DIR, \"sales_monthly.csv\")\n",
    "sales_monthly.to_csv(sales_monthly_path, index=False)\n",
    "logger.info(f\"Saved monthly sales to CSV: {sales_monthly_path}\")\n",
    "\n",
    "sales_weekly_path = os.path.join(OUTPUT_DIR, \"sales_weekly.csv\")\n",
    "sales_weekly.to_csv(sales_weekly_path, index=False)\n",
    "logger.info(f\"Saved weekly sales to CSV: {sales_weekly_path}\")\n",
    "\n",
    "# 14.4 Write monthly sales summary back to Postgres\n",
    "try:\n",
    "    sales_monthly_db = sales_monthly[[\"order_month\", \"total_sales\", \"total_orders\", \"total_qty\"]].copy()\n",
    "    sales_monthly_db.rename(columns={\"order_month\": \"month\"}, inplace=True)\n",
    "    sales_monthly_db.to_sql(\n",
    "        SUMMARY_TABLE,\n",
    "        engine,\n",
    "        schema=SCHEMA,\n",
    "        if_exists=\"replace\",\n",
    "        index=False\n",
    "    )\n",
    "    logger.info(f\"Written monthly sales summary to Postgres table `{SCHEMA}.{SUMMARY_TABLE}`.\")\n",
    "except Exception as e:\n",
    "    logger.exception(\"Failed to write monthly sales summary to Postgres.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Next Steps & Production Best Practices\n",
    "\n",
    "1. **Parameterize & Schedule**: Use Papermill or a similar tool to pass dynamic date ranges or output paths, and schedule this notebook daily/weekly.  \n",
    "2. **Data Versioning**: Store the Parquet files in a date‐partitioned folder (e.g., `/outputs/date=YYYY-MM-DD/`) and push to an object store (S3, GCS) for historical tracking.  \n",
    "3. **CI/CD**: Add unit tests for helper functions (e.g., RFM scoring) using `pytest` or `nbval`. Integrate into your CI pipeline.  \n",
    "4. **Monitoring & Alerting**: Implement checks on key metrics (e.g., total daily sales) and send alerts if they deviate beyond thresholds.  \n",
    "5. **Dashboard Integration**: Use the Postgres summary tables (`monthly_sales_summary`) as data sources for dashboards (Looker, Tableau, Power BI, or Superset).  \n",
    "6. **Scalability**: If the data grows beyond memory, switch to chunked reads or use a database‐side aggregation (e.g., do aggregation in SQL and pull only summary rows).  \n",
    "\n",
    "**End of Notebook**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
